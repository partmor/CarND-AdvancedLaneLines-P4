# **CarND: Advanced Lane Tracking**  [![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)
[//]: # (Image References)

[find_corners]: ./examples/find_corners.png
[test_undistort]: ./examples/test_undistort.png
[original_img]: ./examples/original_img.png
[frame_undistort]: ./examples/frame_undistort.png
[yuv]: ./examples/yuv.png
[luv]: ./examples/luv.png
[lab]: ./examples/lab.png
[L_thresh]: ./examples/L_thresh.png
[B_thresh]: ./examples/B_thresh.png
[L_sobelx]: ./examples/L_sobelx.png
[combined_thresh]: ./examples/combined_thresh.png

The goal of this project is to build a software pipeline to identify the lane boundaries in a video stream generated by a front-facing camera mounted on a vehicle.

## Camera calibration

When a real camera takes 3D objects in the real world and transforms them into a 2D image the transformation is not *perfect*. Lens curvature or lens missalignment with the imaging plane cause distortions (radial or tangential respectively).

In any case, image distortion alters the apparent sizes, shapes, proportion and distances of the real world objects the camera captures.

Calibrating for distortion is performed by taking pictures of known shapes: for instance a **chessboard**. With multiple pictures of a chessboard on a flat surface, it is possible to establish the transformation that maps the position of the corners in the distorted images and their theoretical known values. The parameters of this transformation are the **camera's distortion parameters**.

*[Cells 2 - 8 of the project notebook]*
The chessboard used here has 6 x 9 internal corners. The object points - theoretical real (x,y,z) coordinates in the world - are defined assuming the chessboard is fixed on the (x,y) plane with z = 0:
```python
# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
objp = np.zeros((6*9,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)
```
The calibration images are loaded, converted to grayscale and fed to the `cv2.findChessboardCorners()` method, that detects the internal vertices of the chessboard, and returns their (x,y) position in the image plane.

![find_corners]

Each group of image plane coordinates is paired with their theoretical position, which happens to always be the same `objp` since we are evaluating the same real-world object.

The collection of image points - object points pairs are passed to the `cv2.calibrateCamera()` method that returns the camera's distortion parameters.

Given the latter parameters `cv2.undistort()` allows to undistort any image taken by the camera. Applying this transform on the test image:

![test_undistort]

## Pipeline

Now that the camera has been calibrated, it is possible to start setting up the image pipeline for lane detection. The example worked through this pipeline will be the following image:

![original_img]

This image was chosen since it is a fairly complicated example, due to the irregular shading of the road and the side fence.

### Distortion correction

The distortion correction step has been encapsulated in `aux_fun.undistort()`. This method applied to a video test frame yields:

![frame_undistort]

## Thresholding

*[Cells 10 - 27 of the project notebook]*
The objective is to find colorspaces that are robust *picking* white and yellow lines under changing lighting conditions and tarmac integrity.

The **Lab** colorspace behaved very well in this mission:

![lab]

+ A high-pass filter on the **L channel** picks white lane lines.

![L_thresh]

+ A high-pass filter on the **b channel** helps pick yellow lines, but cannot detect them in low-light conditions.

![B_thresh]

+ Absolute **sobel** thresholding in the **X direction** to the **L channel** picks very well edges of white and yellow lines where the basic color channels seem to fail.

![L_sobelx]

A combined filter using the above three individual filters yields the following mask:

![combined_thresh]

The combined thresholding pipeline has been encapsulated in `thresholding_pipeline()`.

It is worth pointing out that the YUV channel provides very similar results to the Lab: 

![yuv]

In particular, the roles of L and b in the described pipeline could be exchanged by Y and V respectively.




